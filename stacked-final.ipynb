{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold,train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"C:\\Users\\Saiganne\\Documents\\analytics_vidhya\\train.csv\",parse_dates=True)\n",
    "test=pd.read_csv(r\"C:\\Users\\Saiganne\\Documents\\analytics_vidhya\\test.csv\",parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Default (Target:0) 78.29 % of the dataset\n",
      "Default (Target:1) 21.71 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "#checking the class distribution\n",
    "print('No Default (Target:0)', round(train['loan_default'].value_counts()[0]/len(train) * 100,2), '% of the dataset')\n",
    "print('Default (Target:1)', round(train['loan_default'].value_counts()[1]/len(train) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets combine the data for data prep\n",
    "test['loan_default']=np.nan\n",
    "train['data']='train'\n",
    "test['data']='test'\n",
    "test=test[train.columns]\n",
    "all=pd.concat([train,test],axis=0)\n",
    "uid=test['UniqueID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "for col in ['UniqueID','supplier_id','Current_pincode_ID','Employee_code_ID','MobileNo_Avl_Flag']:\n",
    "  all.drop([col],axis=1,inplace=True)\n",
    "  \n",
    "#all['PRIratio_overdue_no.accs']=(all['PRI.OVERDUE.ACCTS']/all['PRI.NO.OF.ACCTS'])\n",
    "#all['PRIratio_overdue_no.accs']=np.where(np.isfinite(all['PRIratio_overdue_no.accs']),all['PRIratio_overdue_no.accs'],0)\n",
    "#all['ratio_dis_sanc']=(all['PRI.DISBURSED.AMOUNT']/all['PRI.SANCTIONED.AMOUNT'])\n",
    "#all['ratio_dis_sanc']=np.where(np.isfinite(all['ratio_dis_sanc']),all['ratio_dis_sanc'],0)\n",
    "#all['ratio_def_acc']=(all['DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS']/all['NEW.ACCTS.IN.LAST.SIX.MONTHS'])\n",
    "#all['ratio_def_acc']=np.where(np.isfinite(all['ratio_def_acc']),all['ratio_def_acc'],0)\n",
    "#all.drop(['PRI.OVERDUE.ACCTS','PRI.NO.OF.ACCTS'],axis=1,inplace=True) dropped this got auc 0.580 instead of 0.581\n",
    "#all['months_to_pay']=round(all['PRI.CURRENT.BALANCE']/all['PRIMARY.INSTAL.AMT'])#total months to clear loan balance\n",
    "#all['balance_amount']=all['PRI.DISBURSED.AMOUNT']-all['PRI.CURRENT.BALANCE']#gives balance ammount to pay for active loans\n",
    "#all['months_to_pay']=np.where(np.isfinite(all['months_to_pay']),all['months_to_pay'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created age column  from Date.of.Birth and dropped Date.of.Birth\n",
    "yr=all['Date.of.Birth'].str.slice(6,8).astype(int)\n",
    "all['age']=np.where(yr==0,(yr+19),((100-yr)+19))\n",
    "all.drop(['Date.of.Birth'],axis=1,inplace=True)\n",
    "\n",
    "# AVERAGE.ACCT.AGE is changed to total num of  months \n",
    "n=all['AVERAGE.ACCT.AGE'].str.replace('yrs',\"-\")\n",
    "n=n.str.replace('mon',\"\")\n",
    "k=n.str.split(\"-\",expand=True).astype(int) #k[0] is years and k[1] gives months\n",
    "all['AVERAGE.ACCT.AGE']=(k[0]*12)+k[1]\n",
    "\n",
    "#CREDIT.HISTORY.LENGTH is changed to total num of months\n",
    "m=all['CREDIT.HISTORY.LENGTH'].str.replace('yrs',\"-\")\n",
    "m=m.str.replace('mon',\"\")\n",
    "k=m.str.split(\"-\",expand=True).astype(int) #k[0] is years and k[1] gives months\n",
    "all['CREDIT.HISTORY.LENGTH']=(k[0]*12)+k[1]\n",
    "\n",
    "#creating dummies for branch_id\n",
    "branch=pd.get_dummies(all['branch_id'],drop_first=True,prefix='branch')\n",
    "all=pd.concat([all,branch],axis=1)\n",
    "all.drop(['branch_id'],axis=1,inplace=True)\n",
    "\n",
    "#creating dummies for manufacturer_id\n",
    "manufac=pd.get_dummies(all['manufacturer_id'],drop_first=True,prefix='manufac')\n",
    "all=pd.concat([all,manufac],axis=1)\n",
    "all.drop(['manufacturer_id'],axis=1,inplace=True)\n",
    "\n",
    "#creating dummies for Employment.Type\n",
    "all[\"Employment.Type\"].fillna(\"Not_employed\", inplace = True)\n",
    "emp=pd.get_dummies(all['Employment.Type'],drop_first=True,prefix='E')\n",
    "all=pd.concat([all,emp],axis=1)\n",
    "all.drop(['Employment.Type'],axis=1,inplace=True)\n",
    "\n",
    "#creating dummies for DisbursalDate\n",
    "all['DisbursalDate']=all['DisbursalDate'].str.slice(3,5).astype(int)\n",
    "disbursal=pd.get_dummies(all['DisbursalDate'],drop_first=True,prefix='Disbursal')\n",
    "all=pd.concat([all,disbursal],axis=1)\n",
    "all.drop(['DisbursalDate'],axis=1,inplace=True)\n",
    "\n",
    "#creating dummies for State_ID\n",
    "state=pd.get_dummies(all['State_ID'],drop_first=True,prefix='State_ID')\n",
    "all=pd.concat([all,state],axis=1)\n",
    "all.drop(['State_ID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranking based on above scenarios\n",
    "all['PERFORM_CNS.SCORE.DESCRIPTION']=all['PERFORM_CNS.SCORE.DESCRIPTION'].map({'Not Scored: Sufficient History Not Available':0,\n",
    "       'Not Scored: Only a Guarantor':0,\n",
    "       'Not Scored: Not Enough Info available on the customer':0,\n",
    "       'Not Scored: No Updates available in last 36 months':0,\n",
    "       'Not Scored: No Activity seen on the customer (Inactive)':0,\n",
    "       'Not Scored: More than 50 active Accounts found':0, \n",
    "       'No Bureau History Available':0, 'M-Very High Risk':1, \n",
    "       'L-Very High Risk':2, 'K-High Risk':3, 'J-High Risk':4, 'I-Medium Risk':5,\n",
    "       'H-Medium Risk':6, 'G-Low Risk':7, 'F-Low Risk':8, 'E-Low Risk':9,\n",
    "      'D-Very Low Risk':10, 'C-Very Low Risk':11, 'B-Very Low Risk':12,\n",
    "      'A-Very Low Risk':13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=all[all['data']=='train']\n",
    "del train['data']\n",
    "test=all[all['data']=='test']\n",
    "test.drop(['loan_default','data'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(train)\n",
    "test=pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233154, 149)\n",
      "(112392, 148)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train.drop(['loan_default'],axis=1)\n",
    "y_train=train['loan_default']\n",
    "x_test=test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincol=x_train.columns\n",
    "testcol=x_test.columns\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "scaler.fit(x_test)\n",
    "\n",
    "x_train=scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "\n",
    "x_train = pd.DataFrame(x_train)\n",
    "x_test=pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
    "            max_depth=8, max_features=0.75, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\n",
    "clf2=LogisticRegression(C=0.05, class_weight='balanced', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
    "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "Algos=[clf1,clf2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233154"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows=x_train.shape[0]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1=pd.DataFrame({'clf1':np.zeros(rows),'clf2':np.zeros(rows)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number :  1\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  2\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  3\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  4\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  5\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  6\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  7\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  8\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  9\n",
      "Algo number : 1\n",
      "Algo number : 2\n",
      "fold number :  10\n",
      "Algo number : 1\n",
      "Algo number : 2\n"
     ]
    }
   ],
   "source": [
    "fold=1\n",
    "\n",
    "for train,left_out_chunk in kf.split(x_train,y_train):\n",
    "    print('fold number : ', fold)\n",
    "    \n",
    "    for i,clf in enumerate(Algos):\n",
    "        print('Algo number :',i+1)\n",
    "        \n",
    "        x_train_train=x_train.loc[train]\n",
    "        y_train_train=y_train[train]\n",
    "        x_train_left_out_chunk=x_train.loc[left_out_chunk]\n",
    "        \n",
    "        clf.fit(x_train_train,y_train_train)\n",
    "        p=clf.predict_proba(x_train_left_out_chunk)[:,1]\n",
    "        \n",
    "        layer1.iloc[left_out_chunk,i]=p\n",
    "        \n",
    "    fold+=1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=x_test.shape[0]\n",
    "layer2_test=pd.DataFrame({'clf1':np.zeros(rows),'clf2':np.zeros(rows)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo number 1\n",
      "Algo number 2\n"
     ]
    }
   ],
   "source": [
    "for i,clf in enumerate(Algos):\n",
    "    print( 'Algo number',i+1)\n",
    "    clf.fit(x_train,y_train)\n",
    "    p=clf.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    layer2_test.iloc[:,i]=p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second layer linear model \n",
    "logr=LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.fit(layer1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred=logr.predict_proba(layer2_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred=pd.DataFrame(final_pred)\n",
    "final_pred.columns=['loan_default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred=pd.concat([uid,final_pred],axis=1)\n",
    "pd.DataFrame(final_pred).to_csv(\"analtics_vidhya.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
